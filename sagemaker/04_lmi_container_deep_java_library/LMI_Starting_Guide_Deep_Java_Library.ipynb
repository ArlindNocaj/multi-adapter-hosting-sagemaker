{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LMI Starting Guide - Deep Java Library](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/starting-guide.html)\n",
    "\n",
    "Starting with v0.27.0, the only required configuration for LMI containers is the HF_MODEL_ID environment variable. LMI will apply optimizations and configurations based on the model architecture and available hardware, removing the need manually set them.\n",
    "\n",
    "Based on the selected container, LMI will automatically:\n",
    "\n",
    "- select the best backend based on the model architecture\n",
    "- enable continuous batching if supported for the model architecture to increase throughput\n",
    "- configure the engine and operation mode\n",
    "- maximize hardware use through tensor parallelism\n",
    "- calculate maximum possible tokens and size the KV-Cache\n",
    "- enable CUDA kernels and optimizations based on the available hardware and drivers\n",
    "\n",
    "The following code example demonstrates this configuration UX using the SageMaker Python SDK.\n",
    "\n",
    "This example will use the TheBloke/Llama2-7b-fp16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.2 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.34.140 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker boto3 awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Assumes SageMaker Python SDK is installed. For example: \"pip install sagemaker\"\n",
    "import sagemaker\n",
    "from sagemaker import image_uris, Model, Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup role and sagemaker session\n",
    "iam_role = sagemaker.get_execution_role() \n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session._region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the uri of the LMI container that supports vLLM, LMI-Dist, HuggingFace Accelerate backends\n",
    "lmi_image_uri = image_uris.retrieve(framework=\"djl-lmi\", version=\"0.28.0\", region=region)\n",
    "\n",
    "# Create the SageMaker Model object. In this example we let LMI configure the deployment settings based on the model architecture  \n",
    "model = Model(\n",
    "  image_uri=lmi_image_uri,\n",
    "  role=iam_role,\n",
    "  env={\n",
    "    \"HF_MODEL_ID\": \"TheBloke/Llama-2-7B-fp16\",\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "# Deploy your model to a SageMaker Endpoint and create a Predictor to make inference requests\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"llama-7b-endpoint\")\n",
    "model.deploy(instance_type=\"ml.g5.2xlarge\", initial_instance_count=1, endpoint_name=endpoint_name)\n",
    "predictor = Predictor(\n",
    "  endpoint_name=endpoint_name,\n",
    "  sagemaker_session=sagemaker_session,\n",
    "  serializer=JSONSerializer(),\n",
    "  deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an inference request against the llama2-7b endpoint\n",
    "outputs = predictor.predict({\n",
    "  \"inputs\": \"The diamondback terrapin was the first reptile to be\",\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"generated_text\": \" featured in a motion picture, in William A. Beardsley's New England Wonderland in 1917. It is skittish and prone to flight as underarm cover.\\nDiamondback turtles must be allowed to cross roads.\\nCities of Miami Beach and Coral Gables have initiatives to protect hatchling diamondback terrapins. Debbie Cannon from Coral Gables reported that 150 nests were dug in 2015, with almost all needing to be relocated to a safer location. \\u201cIf the mother had not dug out a new site, the hatchlings would have gotten run over, or in this area, they probably would have crawled back underneath the beach toys with which they were buried and suffocated and died,\\u201d said Cannon.\\nAs city advocates organized to save diamondback terrapin hatchlings from becoming roadkill, they entered the thick of the urban-swamp interface. Destructive development, urban sprawl and recreational pollution were seriously affecting the wetlands and forests where turtles were most comfortable. The work done to save the diamond\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(outputs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Model Architectures\n",
    "\n",
    "The following text-generation models are supported for optimized inference with the simplified configuration UX:\n",
    "\n",
    "- Aquila & Aquila2 (BAAI/AquilaChat2-7B, BAAI/AquilaChat2-34B, BAAI/Aquila-7B, BAAI/AquilaChat-7B, etc.)\n",
    "- Baichuan & Baichuan2 (baichuan-inc/Baichuan2-13B-Chat, baichuan-inc/Baichuan-7B, etc.)\n",
    "- BLOOM (bigscience/bloom, bigscience/bloomz, etc.)\n",
    "- ChatGLM (THUDM/chatglm2-6b, THUDM/chatglm3-6b, etc.)\n",
    "- DBRX (databricks/dbrx-base, databricks/dbrx-instruct, etc.)\n",
    "- DeciLM (Deci/DeciLM-7B, Deci/DeciLM-7B-instruct, etc.)\n",
    "- Falcon (tiiuae/falcon-7b, tiiuae/falcon-40b, tiiuae/falcon-rw-7b, etc.)\n",
    "- Gemma (google/gemma-2b, google/gemma-7b, etc.)\n",
    "- GPT-2 (gpt2, gpt2-xl, etc.)\n",
    "- GPT BigCode (bigcode/starcoder, bigcode/gpt_bigcode-santacoder, etc.)\n",
    "- GPT-J (EleutherAI/gpt-j-6b, nomic-ai/gpt4all-j, etc.)\n",
    "- GPT-NeoX (EleutherAI/gpt-neox-20b, databricks/dolly-v2-12b, stabilityai/stablelm-tuned-alpha-7b, etc.)\n",
    "- InternLM (internlm/internlm-7b, internlm/internlm-chat-7b, etc.)\n",
    "- LLaMA & LLaMA-2 (meta-llama/Llama-2-70b-hf, lmsys/vicuna-13b-v1.3, young-geng/koala, openlm-research/open_llama_13b, etc.)\n",
    "- Mistral (mistralai/Mistral-7B-v0.1, mistralai/Mistral-7B-Instruct-v0.1, etc.)\n",
    "- Mixtral (mistralai/Mixtral-8x7B-v0.1, mistralai/Mixtral-8x7B-Instruct-v0.1, etc.)\n",
    "- MPT (mosaicml/mpt-7b, mosaicml/mpt-30b, etc.)\n",
    "- OPT (facebook/opt-66b, facebook/opt-iml-max-30b, etc.)\n",
    "- Phi (microsoft/phi-1_5, microsoft/phi-2, etc.)\n",
    "- Qwen (Qwen/Qwen-7B, Qwen/Qwen-7B-Chat, etc.)\n",
    "- Qwen2 (Qwen/Qwen2-beta-7B, Qwen/Qwen2-beta-7B-Chat, etc.)\n",
    "- Yi (01-ai/Yi-6B, 01-ai/Yi-34B, etc.)\n",
    "- T5 (google/flan-t5-xxl, google/flan-t5-base, etc.)\n",
    "\n",
    "Other text-generation models, and non-text generation models are also supported, but may not be as performant as the model architectures listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Environment Variable Configurations\n",
    "\n",
    "The following environment variables are exposed as part of this simplified UX:\n",
    "\n",
    "### HF_MODEL_ID\n",
    "\n",
    "This configuration is used to specify the location of your model artifacts. It can either be a HuggingFace Hub model-id (e.g. TheBloke/Llama-2-7B-fp16), a S3 uri (e.g. s3://my-bucket/my-model/), or a local path. If you are using SageMaker's capability to specify uncompressed model artifacts, you should set this value to /opt/ml/model. /opt/ml/model is the path in the container where model artifacts are mounted if using this mechanism.\n",
    "\n",
    "### HF_REVISION\n",
    "\n",
    "If you are using a model from the HuggingFace Hub, this specifies the commit or branch to use when downloading the model.\n",
    "\n",
    "This is an optional config, and does not have a default value.\n",
    "\n",
    "### HF_TOKEN\n",
    "\n",
    "Some models on the HuggingFace Hub are gated and require permission from the owner to access. To deploy a gated model from the HuggingFace Hub using LMI, you must provide an Access Token via this environment variable.\n",
    "\n",
    "### HF_MODEL_TRUST_REMOTE_CODE\n",
    "\n",
    "If the model artifacts contain custom modeling code, you should set this to true after validating the custom code is not malicious. If you are using a HuggingFace Hub model id, you should also specify HF_REVISION to ensure you are using artifacts and code that you have validated.\n",
    "\n",
    "This is an optional config, and defaults to False.\n",
    "\n",
    "### TENSOR_PARALLEL_DEGREE\n",
    "\n",
    "This value is used to specify the number of GPUs to partition the model across using tensor parallelism. The value should be less than or equal to the number of available GPUs on the instance type you are deploying with. We recommend setting this to max, which will shard the model across all available GPUs.\n",
    "\n",
    "This is an optional config, and defaults to max.\n",
    "\n",
    "### MAX_BATCH_SIZE\n",
    "\n",
    "This value represents the maximum number of requests the model will handle in a batch.\n",
    "\n",
    "This is an optional config, and defaults to 256.\n",
    "\n",
    "### MAX_CONCURRENT_REQUESTS\n",
    "\n",
    "This value represents the number of active requests/client connections the model server can handle. This value should be greater than or equal to MAX_BATCH_SIZE. For requests received when a full batch is being executed, they will be queued until a free slot in the batch becomes available.\n",
    "\n",
    "This is an optional config, and defaults to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Configurations\n",
    "\n",
    "Additional configurations are available to further tune and customize your deployment. These configurations are covered as part of the advanced deployment guide here. In most cases, the simplified configuration set described here is sufficient to achieve high performance. We recommend you first deploy using the configurations described here, and venture into the advanced configurations if you need additional customizations and performance tuning.\n",
    "\n",
    "The configurations described in that doc follow a different naming convention when using the serving.properties configuration format. The above environment variables translate to the following serving.properties configurations:\n",
    "\n",
    "- HF_MODEL_ID: option.model_id\n",
    "- HF_REVISION: option.revision\n",
    "- HF_MODEL_TRUST_REMOTE_CODE: option.trust_remote_code\n",
    "- TENSOR_PARALLEL_DEGREE: option.tensor_parallel_degree\n",
    "- MAX_BATCH_SIZE: option.max_rolling_batch_size\n",
    "- MAX_CONCURRENT_REQUESTS: job_queue_size\n",
    "\n",
    "## API Schema\n",
    "\n",
    "The request and response schema for interacting with the model endpoint is available [here](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/lmi_input_output_schema.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
