{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844f22ae-6b01-4a17-820b-83f9d34de23e",
   "metadata": {},
   "source": [
    "# Serve multiple LoRA adapters efficiently on SageMaker - vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cd38c-ad40-4a3e-b225-4d1f707a0c0e",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to serve many Low-Rank Adapters (LoRA) on top of the same base model efficiently on the same GPU. In order to do this, we'll deploy a [vLLM serving-based container](https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html) to SageMaker Hosting. \n",
    "\n",
    "These are the steps we will take:\n",
    "\n",
    "1. [Setup our environment](#setup)\n",
    "2. [Build a new vLLM container image compatible with SageMaker, push it to Amazon ECR](#container)\n",
    "3. [Download adapters from the HuggingFace Hub and upload them to S3](#download_adapter)\n",
    "4. [Build LoRA modules manifest file](#manifest)\n",
    "5. [Deploy the extended vLLM container to SageMaker](#deploy)\n",
    "6. [Compare outputs of the base model and the adapter model](#compare)\n",
    "7. [Benchmark our deployed endpoint under different traffic patterns - same adapter, and random access to many adapters](#benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65e8be-3073-4436-b311-11a3de09e101",
   "metadata": {},
   "source": [
    "## What is vLLM? \n",
    "\n",
    "vLLM is a fast and easy-to-use library for LLM inference and serving. It supports most state-of-the art LLM serving optimizations, such as PagedAttention, FlashAttention, continuous batching and more. \n",
    "\n",
    "Recently vLLM added support for efficient multi-LoRA serving, with one of the key features being support for different LoRA ranks in the same batch. This is important for users that tune each adapter's rank to its specific task and dataset to get the best overall performance (although the need for this is not definitive, see [here](https://arxiv.org/abs/2402.09353)).\n",
    "\n",
    "You can read more about vLLM and its multi-LoRA serving feature [here](https://docs.vllm.ai/en/latest/models/lora.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4372522-fb5d-4cad-be37-d4f21f794698",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf3acc-e2fc-48ca-a21e-4377c7638d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8394b-5d69-4c02-97fe-d00eccc4da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d6fd3-5d59-4bc2-8cf2-ad4c32d072b7",
   "metadata": {},
   "source": [
    "<a id=\"container\"></a>\n",
    "## Build a new vLLM container image compatible with SageMaker, push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9d085-897f-46a4-875f-798f0ebb1f61",
   "metadata": {},
   "source": [
    "This example includes a `Dockerfile` and `sagemaker_entrypoint.sh` in the `sagemaker_vllm` directory. Building this new container image makes vLLM compatible with SageMaker Hosting, namely launching the server on port 8080 via the container's `ENTRYPOINT` instruction, and changing the relevant server routes from the original `/ping` and `/v1/completions` to `/health` and `/invocations` . [Here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) you can find the basic interfaces required to adapt any container for deployment on Sagemaker Hosting.\n",
    "\n",
    "We also make sure all relevant server configuration arguments (such as enabling the LoRA serving feature, LoRA module paths, etc.) are configurable via environment variables, so that our container entrypoint can be parametrized at runtime by SageMaker. vLLM does not support env var-based server config by default. You can find all possible configuration parameters within vllm [Server args](https://github.com/vllm-project/vllm/blob/865732342b4e3b8a4ef38f28a2a5bdb87cf3f970/vllm/entrypoints/openai/cli_args.py#L25) and [AsyncEngine args](https://github.com/vllm-project/vllm/blob/865732342b4e3b8a4ef38f28a2a5bdb87cf3f970/vllm/engine/arg_utils.py#L12); if you want to expose other parameters, add them to the Dockerfile, and make sure to pass them when launching the SageMaker Endpoint, as we will see in the next sections. A relevant one to expose for larger models would be [--tensor-parallel-size](https://github.com/vllm-project/vllm/blob/865732342b4e3b8a4ef38f28a2a5bdb87cf3f970/vllm/engine/arg_utils.py#L164C30-L164C52).\n",
    "\n",
    "Let's analyze the Dockerfile and entrypoint script to understand how easy it is to adapt any serving framework (and vLLM in particula) to run on SageMaker Real-Time Hosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0eafe8-2398-43e0-9dd6-e211c23ba5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize sagemaker_vllm/sagemaker_entrypoint.sh\n",
    "!printf \"\\n\\n\\nEnd of entrypoint script ----------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0284de-24fa-4d07-852a-65e5a4e701c4",
   "metadata": {},
   "source": [
    "Here are the relevant things to note from the previous cell output:\n",
    "1. we force the server to run on port 8080, as required by SageMaker\n",
    "2. selected base model is parameterized with `HF_MODEL_ID` env var\n",
    "3. maximum allowed sequence lenght (input+output) is parameterized with `MAX_MODEL_LEN` env var; this is an important parameter, as many models' max len is larger than what a single A10G can hold, which would cause the server to error and exit at startup\n",
    "4. maximum number of LoRA adapters that can run within the same batch on the GPU are parameterized with `MAX_GPU_LORAS` env var; the default in vLLM is 1, which would provide poor performance. To define an appropriate value for this parameter, you should take into consideration the total memory of the GPU you will be deploying on, memory required for each adapter, and the expected input/output lengths of the incoming payloads\n",
    "5. maximum number of LoRA adapters that can be offloaded to CPU memory (RAM) for quick hotswapping is parameterized with `MAX_CPU_LORAS`. To define an appropriate value for this parameter, you should take into consideration the total RAM available in the instance type you will deploy on, and memory required for each adapter\n",
    "6. maximum number of sequences that can be processed per iteration is parameterized with `MAX_NUM_SEQS`; it's important to tailor this to the GPU being used, as some GPU memory is pre-allocated based on its value\n",
    "7. whether to enforce eager or not is parameterizes with `ENFORCE_EAGER`; by default vLLM captures the model for CUDA graphs which reduces its latency, but it also consumes an extra 1-3GB of memory, so it can be turned off by enforcing eager mode\n",
    "8. the names (invocation target ids) and local paths for all LoRA adapters and their artifacts are listed within a manifest file (we will construct it later according to vLLM's `--lora-modules` [arg specification](https://docs.vllm.ai/en/latest/models/lora.html#serving-lora-adapters)), the name and directory of which we pass via the `LORA_MODULES_MANIFEST_FILE` and `MODEL_DIR` env vars\n",
    "\n",
    "**Why do we have to pass all local directories for adapter artifacts in 8.?** --> At the time of writing, vLLM's LoRA serving feature does not allow for dynamic downloads of LoRA adapters from S3 or HF Hub as they are invoked. All adapters must be present locally on the underlying instance that the server runs on. That does not mean you have to include all the adapter artifacts in your container image, as this would be very rigid and unfriendly for image reusability. We will show you how downloading adapters from S3 can be done dynamically before the server starts up with the help of Sagemaker in the next sections.\n",
    "\n",
    "**Why a manifest file instead of just another environment variable?** --> SageMaker enforces the length of the json encoded env vars dictionary that is passed to be under 1024 characters. This might not be enough to build the `--lora-modules` argument, especially as the number of adapters (i.e. modules) grows. With this in mind, we will build a manifest file that is downloaded and read into a variable before the vLLM server is started, sidestepping this limitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823fa62-cb48-47e3-9668-752806a3592d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ The above approach is specific to the latest version of the vLLM container at the time of writing, and will likely change with updates to vLLM.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cc01a-13e6-4d6e-9637-00dc2e94050d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize sagemaker_vllm/Dockerfile\n",
    "!printf \"\\n\\n\\nEnd of Dockerfile ----------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e77e4f-5a19-476a-a0a8-d93141a174f3",
   "metadata": {},
   "source": [
    "In the output of the above cell, you can see that we:\n",
    "* fix to a specific vLLM container version, which we pass when we build the image\n",
    "* replace the `/health` and `/v1/completions` server routes by `/ping` and `/invocations` in the main server launch script, as required by SageMaker\n",
    "* copy our entrypoint script to the container, and set it as the ENTRYPOINT command, as required by SageMaker\n",
    "\n",
    "! NOTE !: if you change the vLLM base container version, check to make sure the string replacements above still work as intended, and the path to the main server launch script still holds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0c937-9e17-42ea-afc5-31d5dbff6331",
   "metadata": {},
   "source": [
    "We are good to go! We build the new container image and push it to a new ECR repository. Note SageMaker [supports private Docker registries](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02779726-b57f-472b-9d09-6a1aabc56a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s {region}\n",
    "algorithm_name=\"sagemaker-vllm\"  # name of your algorithm\n",
    "tag=\"v0.3.3\"\n",
    "region=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "image_uri=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" --region $region > /dev/null\n",
    "fi\n",
    "\n",
    "cd sagemaker_vllm/ && docker build --build-arg VERSION=$tag -t ${algorithm_name}:${tag} .\n",
    "\n",
    "# Authenticate Docker to an Amazon ECR registry\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Tag the image\n",
    "docker tag ${algorithm_name}:${tag} ${image_uri}\n",
    "\n",
    "# Push the image to the repository\n",
    "docker push ${image_uri}\n",
    "\n",
    "# Save image name to tmp file to use when deploying endpoint\n",
    "echo $image_uri > /tmp/image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d38628-9317-408f-906d-e5c9ce80fbb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"download_adapter\"></a>\n",
    "## Download adapter from HuggingFace Hub and push it to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6cb518-220c-491f-ae30-50f14f2b4ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are going to simulate storing our adapter weights on S3, and having SageMaker download them upfront when we provision the endpoint. This enables most scenarios, including deployment after you’ve finetuned your own adapters and pushed them to S3, as well as securing deployments with no internet access inside your VPC, as detailed in this [blog post](https://www.philschmid.de/sagemaker-llm-vpc#2-upload-the-model-to-amazon-s3).\n",
    "\n",
    "We first download an adapter trained with Mistral Instruct v0.1 as the base model to a local directory. This particular adapter was trained on GSM8K, a grade school math dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3230c3b-7740-4727-b8a1-eda109afe077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n",
    "# create model dir\n",
    "model_dir = Path('mistral-adapter')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(\n",
    "    HF_MODEL_ID,\n",
    "    local_dir=str(model_dir), # download to model dir\n",
    "    local_dir_use_symlinks=False, # use no symlinks to save disk space\n",
    "    revision=\"main\", # use a specific revision, e.g. refs/pr/21\n",
    "    cache_dir='/home/ec2-user/SageMaker/.cache/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b00f3-ca40-4b87-9456-71df3e6efe95",
   "metadata": {},
   "source": [
    "We copy this same adapter `n_adapters` times to different S3 prefixes in our SageMaker session bucket, simulating a large number of adapters we want to serve on the same endpoint and underlying GPU. We name the last prefix directory (leaf directory) as integer indexes, but you can change this to reflect the task or name of each adapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a490c-a540-4e0d-8a44-a8b886bbfac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_folder_to_s3(local_path, s3_bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_object_key = os.path.join(s3_prefix, os.path.relpath(local_file_path, local_path))\n",
    "            s3.upload_file(local_file_path, s3_bucket, s3_object_key)\n",
    "\n",
    "# Upload the folder n_adapters times under different prefixes\n",
    "n_adapters=50\n",
    "base_prefix = 'vllm/mistral-adapters'\n",
    "for i in range(1, n_adapters+1):\n",
    "    prefix = f'{base_prefix}/{i}'\n",
    "    upload_folder_to_s3(model_dir, sagemaker_session_bucket, prefix)\n",
    "    print(f'Uploaded folder to S3 with prefix: {prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aadcf5-dc32-4bc8-955b-16d4011a19ab",
   "metadata": {},
   "source": [
    "<a id=\"manifest\"></a>\n",
    "## Build LoRA modules manifest file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e454773-7a38-41cc-bb84-547550f6ee78",
   "metadata": {},
   "source": [
    "Now, we build the manifest file. The `--lora-modules` arg syntax to follow is `MODULE_NAME_1=LOCAL_DIRECTORY_1 MODULE_NAME_2=LOCAL_DIRECTORY_2 ...`. \n",
    "\n",
    "To maintain consistency and ease of management, we want our module names (which we will invoke later) to match the names of the leaf directories on S3 that hold each adapter artifact. In order to do this, we list the contents of our `base_prefix` directory on S3, parse the leaf directory names, and use them to build the `--lora-modules` argument. In this case, the leaf names will be integers in the range of 1 to `n_adapters`; however, the following cells can be reused for any S3 prefix that holds directories with LoRA artifacts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d04b0-63fa-4424-a3a4-177688c16023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_s3_leaf_directories(bucket_name, prefix):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "\n",
    "    leaf_directories = []\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        directory_path = prefix['Prefix']\n",
    "        directory_name = directory_path.rstrip('/').rsplit('/', 1)[-1]  # Extract the last part of the path, the leaf directory\n",
    "        leaf_directories.append(directory_name)\n",
    "\n",
    "    return leaf_directories\n",
    "\n",
    "\n",
    "bucket_name = sagemaker_session_bucket\n",
    "prefix = base_prefix+'/'\n",
    "leaf_directories = list_s3_directories(bucket_name, prefix)\n",
    "print(leaf_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913ed58-a7db-4fc1-8bd8-3f6da1767d75",
   "metadata": {},
   "source": [
    "Now, we build the manifest file and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8a9cc-cff9-4229-96e2-c6856b582370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_modules_manifest_file = \"lora_modules.txt\"\n",
    "\n",
    "with open(lora_modules_manifest_file, \"w\") as file:\n",
    "    vllm_arg_pattern = \" \".join([f\"{leaf_dir}=/opt/ml/model/{leaf_dir}/\" for leaf_dir in leaf_directories])\n",
    "    file.write(vllm_arg_pattern)\n",
    "\n",
    "# Upload to S3\n",
    "s3.upload_file(lora_modules_file, sagemaker_session_bucket, f'{base_prefix}/{lora_modules_manifest_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383850b2-6e87-4157-be5e-bf4b52670a25",
   "metadata": {},
   "source": [
    "We can verify the file was uploaded to the correct directory (our base_prefix) on the following cell. If there is no output, the file is not where it should be, and you should verify the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83086429-632b-40b4-aa33-20e24bd3ca99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 ls \"s3://$sagemaker_session_bucket/$base_prefix/\" | grep \"$lora_modules_manifest_file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ed1b2-8b7c-4e9b-bace-7ace5f78e1ca",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## Deploy SageMaker endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24eae-766a-4ec7-8ecf-de45f1092051",
   "metadata": {},
   "source": [
    "Now we deploy a SageMaker endpoint, pointing to our `base_prefix` as the `model_data` parameter.\n",
    "\n",
    "Let's dissect what is happening here:\n",
    "* as explained in the SageMaker [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts), SageMaker downloads model artifacts under the provided `S3URI` to the `/opt/ml/model` directory; your container has read-only access to this directory\n",
    "* by specifying that our data is in an `S3Prefix` and `CompressionType` is `None`, you do not need to tar.gz the `base_prefix` directory; SageMaker will download all the files and directories in our `base_prefix` in uncompressed format, replicating the S3 directory structure (i.e. the `base_prefix` dir structure will match the `/opt/ml/model` dir structure). This is why we pass `/opt/ml/model` as the `MODEL_DIR` in the next cell, and why we placed the lora manifest file in the root of the `base_prefix` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02925d2b-147c-4c84-af40-cc7c32fa7993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "from sagemaker import Model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Retrieve image_uri from tmp file\n",
    "image_uri = !cat /tmp/image_uri\n",
    "# Increased health check timeout to give time for model download\n",
    "health_check_timeout = 800\n",
    "# Endpoint configs\n",
    "number_of_gpu = 1\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sm-vllm\")\n",
    "\n",
    "# Env vars required for server launch\n",
    "config = {\n",
    "  'MODEL_DIR': '/opt/ml/model', # root dir for adatper dirs and manifest file\n",
    "  'LORA_MODULES_MANIFEST_FILE': lora_modules_manifest_file, # manifest file name\n",
    "  'HF_MODEL_ID': \"mistralai/Mistral-7B-Instruct-v0.1\", # model_id from hf.co/models\n",
    "  'MAX_MODEL_LEN': json.dumps(4096),  # max length of input text\n",
    "  'MAX_GPU_LORAS': json.dumps(20), # max number of adapters usable in single batch\n",
    "  'MAX_CPU_LORAS': json.dumps(50), # max number of adapters that can be held in CPU mem\n",
    "  'MAX_NUM_SEQS': json.dumps(100), # max number of sequences per iteration\n",
    "  'ENFORCE_EAGER': json.dumps(False), # whether to turn off CUDA graphs and enforce eager mode (saves GPU mem) \n",
    "}\n",
    "\n",
    "# Create SM Model, pass in model data as a whole prefix of uncompressed model artifacts\n",
    "vllm_model = Model(\n",
    "    image_uri=image_uri[0],\n",
    "    model_data={\n",
    "        'S3DataSource':{\n",
    "            'S3Uri': f's3://{sagemaker_session_bucket}/{base_prefix}/',\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'CompressionType': 'None'}},\n",
    "    env=config,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "vllm_predictor = vllm_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c9e7a-8a1e-4b02-bc60-77f87f7aa5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can reinstantiate the Predictor object if you restart the notebook or Predictor is None\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "endpoint_name = endpoint_name\n",
    "\n",
    "vllm_predictor = Predictor(\n",
    "    endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308c12e-365c-4737-90f5-998f849beffe",
   "metadata": {},
   "source": [
    "<a id=\"compare\"></a>\n",
    "## Invoke base model and adapter, compare outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ae3e2-8634-4e4d-957a-a3d81d7490e7",
   "metadata": {},
   "source": [
    "We can invoke the base Mistral model, as well as any of the adapters downloaded to our endpoint! vLLM will take care of downloading them, continuously batch requests for different adapters, and manage DRAM and RAM by loading/offloading adapters.\n",
    "\n",
    "Let’s inspect the difference between the base model’s response and the adapter’s response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167babae-0fb9-4a7f-aba9-fe4e266ebdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "\n",
    "payload_base = {\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"temperature\":0\n",
    "}\n",
    "\n",
    "\n",
    "payload_adapter = {\n",
    "    \"model\": \"1\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"temperature\":0\n",
    "}\n",
    "\n",
    "response_base = vllm_predictor.predict(payload_base)\n",
    "response_adapter = vllm_predictor.predict(payload_adapter)\n",
    "\n",
    "print(f'Base model output:\\n-------------\\n {response_base[\"choices\"][0][\"text\"]}')\n",
    "print(f'\\nAdapter output:\\n-------------\\n {response_adapter[\"choices\"][0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd03354-75e8-47e9-91e8-dbf52d95d629",
   "metadata": {},
   "source": [
    "You can also check out the full details of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45874488-5ffb-41bb-9bd5-d7f7343928a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1010056-9b46-4c62-880c-9f4552aa2af7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## Benchmark single adapter vs. random access to adapters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67667c2d-c154-42dc-9be7-d577f8855e1d",
   "metadata": {},
   "source": [
    "First, we individually call each of the adapters in sequence, to make sure they are previously loaded to either GPU or CPU memory. We want to exclude disk read latency from the benchmark metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeea08-31e8-411c-a57b-c4acdd702f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(1,n_adapters+1)):\n",
    "    payload_adapter = {\n",
    "        \"model\": str(i),\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 64,\n",
    "        \"temperature\":0\n",
    "    }\n",
    "    vllm_predictor.predict(payload_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eeb399-7490-4697-9871-777038232200",
   "metadata": {},
   "source": [
    "Now we are ready to benchmark. For the single adapter case, we invoke the adapter `total_requests` times from `num_threads` concurrent clients.\n",
    "\n",
    "For the multi-adapter case, we invoke a random adapter from any of the clients, until all adapters have been invoked `total_requests//num_adapters` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520c962-f064-4e8a-a9ce-2405084e3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust if you run into connection pool errors\n",
    "# import botocore\n",
    "\n",
    "# Configure botocore to use a larger connection pool\n",
    "# config = botocore.config.Config(max_pool_connections=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94ddaa-4d57-4139-8e76-b87593fb155e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Configuration\n",
    "total_requests = 300\n",
    "num_adapters = 50\n",
    "num_threads = 20  # Adjust based on your system capabilities\n",
    "\n",
    "\n",
    "# Shared lock and counters for # invocations of each adapter \n",
    "adapter_counters = [total_requests // num_adapters] * num_adapters\n",
    "counters_lock = threading.Lock()\n",
    "\n",
    "def invoke_adapter(aggregate_latency, single_adapter=False):\n",
    "    global total_requests\n",
    "    latencies = []\n",
    "    while True:\n",
    "        with counters_lock:\n",
    "            if single_adapter:\n",
    "                adapter_id = 1\n",
    "                if total_requests > 0:\n",
    "                    total_requests -= 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                # Find an adapter that still needs to be called\n",
    "                remaining_adapters = [i for i, count in enumerate(adapter_counters) if count > 0]\n",
    "                if not remaining_adapters:\n",
    "                    break\n",
    "                adapter_id = random.choice(remaining_adapters) + 1\n",
    "                adapter_counters[adapter_id - 1] -= 1\n",
    "\n",
    "        prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "\n",
    "        payload_adapter = {\n",
    "            \"model\": str(adapter_id),\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 64,\n",
    "            \"temperature\":0\n",
    "        }\n",
    "        start_time = time.time()\n",
    "        response_adapter = vllm_predictor.predict(payload_adapter)\n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    aggregate_latency.extend(latencies)\n",
    "\n",
    "def benchmark_scenario(single_adapter=False):\n",
    "    threads = []\n",
    "    all_latencies = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(num_threads):\n",
    "        thread_latencies = []\n",
    "        all_latencies.append(thread_latencies)\n",
    "        thread = threading.Thread(target=invoke_adapter, args=(thread_latencies, single_adapter))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    total_latency = sum([sum(latencies) for latencies in all_latencies])\n",
    "    total_requests_made = sum([len(latencies) for latencies in all_latencies])\n",
    "    average_latency = total_latency / total_requests_made\n",
    "    throughput = total_requests_made / (time.time() - start_time)\n",
    "\n",
    "    print(f\"Total Time: {time.time() - start_time}s\")\n",
    "    print(f\"Average Latency: {average_latency}s\")\n",
    "    print(f\"Throughput: {throughput} requests/s\")\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Benchmarking: Single Adapter Multiple Times\")\n",
    "benchmark_scenario(single_adapter=True)\n",
    "\n",
    "print(\"\\nBenchmarking: Multiple Adapters with Random Access\")\n",
    "benchmark_scenario()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9521-f5ed-487b-ba84-5a834add3d78",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "## Cleanup endpoint resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920da7-c0ce-4f3f-9eea-297abb49b342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vllm_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
