{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844f22ae-6b01-4a17-820b-83f9d34de23e",
   "metadata": {},
   "source": [
    "# Serve multiple LoRA adapters efficiently on SageMaker - vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cd38c-ad40-4a3e-b225-4d1f707a0c0e",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to serve many Low-Rank Adapters (LoRA) on top of the same base model efficiently on the same GPU. In order to do this, we'll deploy a [vLLM serving-based container](https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html) to SageMaker Hosting. \n",
    "\n",
    "These are the steps we will take:\n",
    "\n",
    "1. [Setup our environment](#setup)\n",
    "2. [Build a new vLLM container image compatible with SageMaker, push it to Amazon ECR](#container)\n",
    "3. [Download adapters from the HuggingFace Hub and upload them to S3](#download_adapter)\n",
    "4. [Build LoRA modules manifest file](#manifest)\n",
    "5. [Deploy the extended vLLM container to SageMaker](#deploy)\n",
    "6. [Compare outputs of the base model and the adapter model](#compare)\n",
    "7. [Benchmark our deployed endpoint under different traffic patterns - same adapter, and random access to many adapters](#benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65e8be-3073-4436-b311-11a3de09e101",
   "metadata": {},
   "source": [
    "## What is vLLM? \n",
    "\n",
    "vLLM is a fast and easy-to-use library for LLM inference and serving. It supports most state-of-the art LLM serving optimizations, such as PagedAttention, FlashAttention, continuous batching and more. \n",
    "\n",
    "Recently vLLM added support for efficient multi-LoRA serving, with one of the key features being support for different LoRA ranks in the same batch. This is important for users that tune each adapter's rank to its specific task and dataset to get the best overall performance (although the need for this is not definitive, see [here](https://arxiv.org/abs/2402.09353)).\n",
    "\n",
    "You can read more about vLLM and its multi-LoRA serving feature [here](https://docs.vllm.ai/en/latest/models/lora.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4372522-fb5d-4cad-be37-d4f21f794698",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdf3acc-e2fc-48ca-a21e-4377c7638d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d8394b-5d69-4c02-97fe-d00eccc4da26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker default S3 bucket: sagemaker-us-east-1-626723862963\n",
      "sagemaker role arn: arn:aws:iam::626723862963:role/service-role/AmazonSageMaker-ExecutionRole-20231214T145077\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker default S3 bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d6fd3-5d59-4bc2-8cf2-ad4c32d072b7",
   "metadata": {},
   "source": [
    "<a id=\"container\"></a>\n",
    "## Build a new vLLM container image compatible with SageMaker, push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9d085-897f-46a4-875f-798f0ebb1f61",
   "metadata": {},
   "source": [
    "This example includes a `Dockerfile` and `sagemaker_entrypoint.sh` in the `sagemaker_vllm` directory. Building this new container image makes vLLM compatible with SageMaker Hosting, namely launching the server on port 8080 via the container's `ENTRYPOINT` instruction, and changing the relevant server routes from the original `/ping` and `/v1/completions` to `/health` and `/invocations` . [Here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) you can find the basic interfaces required to adapt any container for deployment on Sagemaker Hosting.\n",
    "\n",
    "We also make sure all relevant server configuration arguments (such as enabling the LoRA serving feature, LoRA module paths, etc.) are configurable via environment variables, so that our container entrypoint can be parametrized at runtime by SageMaker. vLLM does not support env var-based server config by default. You can find all possible configuration parameters within vllm [Server args](https://github.com/vllm-project/vllm/blob/865732342b4e3b8a4ef38f28a2a5bdb87cf3f970/vllm/entrypoints/openai/cli_args.py#L25) and [AsyncEngine args](https://github.com/vllm-project/vllm/blob/865732342b4e3b8a4ef38f28a2a5bdb87cf3f970/vllm/engine/arg_utils.py#L12); if you want to expose other parameters, add them to the Dockerfile, and make sure to pass them when launching the SageMaker Endpoint, as we will see in the next sections. A relevant one to expose for larger models would be [--tensor-parallel-size](https://github.com/vllm-project/vllm/blob/865732342b4e3b8a4ef38f28a2a5bdb87cf3f970/vllm/engine/arg_utils.py#L164C30-L164C52).\n",
    "\n",
    "Let's analyze the Dockerfile and entrypoint script to understand how easy it is to adapt any serving framework (and vLLM in particula) to run on SageMaker Real-Time Hosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0eafe8-2398-43e0-9dd6-e211c23ba5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/bin/bash\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[31mLORA_MODULES\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00m<\u001b[33m\"\u001b[39;49;00m\u001b[31m$MODEL_DIR\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[31m$LORA_MODULES_MANIFEST_FILE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[31mLAUNCH_COMMAND\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33mvllm.entrypoints.openai.api_server \\\u001b[39;49;00m\n",
      "\u001b[33m--port 8080 \\\u001b[39;49;00m\n",
      "\u001b[33m--model \u001b[39;49;00m\u001b[31m$HF_MODEL_ID\u001b[39;49;00m\u001b[33m \\\u001b[39;49;00m\n",
      "\u001b[33m--max-model-len \u001b[39;49;00m\u001b[31m$MAX_MODEL_LEN\u001b[39;49;00m\u001b[33m \\\u001b[39;49;00m\n",
      "\u001b[33m--enable-lora \\\u001b[39;49;00m\n",
      "\u001b[33m--lora-modules \u001b[39;49;00m\u001b[31m$LORA_MODULES\u001b[39;49;00m\u001b[33m \\\u001b[39;49;00m\n",
      "\u001b[33m--max-loras \u001b[39;49;00m\u001b[31m$MAX_GPU_LORAS\u001b[39;49;00m\u001b[33m \\\u001b[39;49;00m\n",
      "\u001b[33m--max-cpu-loras \u001b[39;49;00m\u001b[31m$MAX_CPU_LORAS\u001b[39;49;00m\u001b[33m \\\u001b[39;49;00m\n",
      "\u001b[33m--max-num-seqs \u001b[39;49;00m\u001b[31m$MAX_NUM_SEQS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Check if ENFORCE_EAGER environment variable is 'true', append to launch command if so\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m\u001b[37m \u001b[39;49;00m[\u001b[37m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[31m$ENFORCE_EAGER\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m\"true\"\u001b[39;49;00m\u001b[37m \u001b[39;49;00m];\u001b[37m \u001b[39;49;00m\u001b[34mthen\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[37m# Append --enforce-eager to the command string\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[31mLAUNCH_COMMAND\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$LAUNCH_COMMAND\u001b[39;49;00m\u001b[33m --enforce-eager\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Launch vLLM\u001b[39;49;00m\n",
      "python3\u001b[37m \u001b[39;49;00m-m\u001b[37m \u001b[39;49;00m\u001b[31m$LAUNCH_COMMAND\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "End of entrypoint script ----------------"
     ]
    }
   ],
   "source": [
    "!pygmentize sagemaker_vllm/sagemaker_entrypoint.sh\n",
    "!printf \"\\n\\n\\nEnd of entrypoint script ----------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0284de-24fa-4d07-852a-65e5a4e701c4",
   "metadata": {},
   "source": [
    "Here are the relevant things to note from the previous cell output:\n",
    "1. we force the server to run on port 8080, as required by SageMaker\n",
    "2. selected base model is parameterized with `HF_MODEL_ID` env var\n",
    "3. maximum allowed sequence lenght (input+output) is parameterized with `MAX_MODEL_LEN` env var; this is an important parameter, as many models' max len is larger than what a single A10G can hold, which would cause the server to error and exit at startup\n",
    "4. maximum number of LoRA adapters that can run within the same batch on the GPU are parameterized with `MAX_GPU_LORAS` env var; the default in vLLM is 1, which would provide poor performance. To define an appropriate value for this parameter, you should take into consideration the total memory of the GPU you will be deploying on, memory required for each adapter, and the expected input/output lengths of the incoming payloads\n",
    "5. maximum number of LoRA adapters that can be offloaded to CPU memory (RAM) for quick hotswapping is parameterized with `MAX_CPU_LORAS`. To define an appropriate value for this parameter, you should take into consideration the total RAM available in the instance type you will deploy on, and memory required for each adapter\n",
    "6. maximum number of sequences that can be processed per iteration is parameterized with `MAX_NUM_SEQS`; it's important to tailor this to the GPU being used, as some GPU memory is pre-allocated based on its value\n",
    "7. whether to enforce eager or not is parameterizes with `ENFORCE_EAGER`; by default vLLM captures the model for CUDA graphs which reduces its latency, but it also consumes an extra 1-3GB of memory, so it can be turned off by enforcing eager mode\n",
    "8. the names (invocation target ids) and local paths for all LoRA adapters and their artifacts are listed within a manifest file (we will construct it later according to vLLM's `--lora-modules` [arg specification](https://docs.vllm.ai/en/latest/models/lora.html#serving-lora-adapters)), the name and directory of which we pass via the `LORA_MODULES_MANIFEST_FILE` and `MODEL_DIR` env vars\n",
    "\n",
    "**Why do we have to pass all local directories for adapter artifacts in 8.?** --> At the time of writing, vLLM's LoRA serving feature does not allow for dynamic downloads of LoRA adapters from S3 or HF Hub as they are invoked. All adapters must be present locally on the underlying instance that the server runs on. That does not mean you have to include all the adapter artifacts in your container image, as this would be very rigid and unfriendly for image reusability. We will show you how downloading adapters from S3 can be done dynamically before the server starts up with the help of Sagemaker in the next sections.\n",
    "\n",
    "**Why a manifest file instead of just another environment variable?** --> SageMaker enforces the length of the json encoded env vars dictionary that is passed to be under 1024 characters. This might not be enough to build the `--lora-modules` argument, especially as the number of adapters (i.e. modules) grows. With this in mind, we will build a manifest file that is downloaded and read into a variable before the vLLM server is started, sidestepping this limitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823fa62-cb48-47e3-9668-752806a3592d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ The above approach is specific to the latest version of the vLLM container at the time of writing, and will likely change with updates to vLLM.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2cc01a-13e6-4d6e-9637-00dc2e94050d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m\u001b[37m \u001b[39;49;00mVERSION\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[33mvllm/vllm-openai:$VERSION\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Make server compatible with SageMaker Hosting contract\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m\u001b[37m \u001b[39;49;00msed\u001b[37m \u001b[39;49;00m-i\u001b[37m \u001b[39;49;00m\u001b[33m's|/health|/ping|g'\u001b[39;49;00m\u001b[37m \u001b[39;49;00mvllm/entrypoints/openai/api_server.py\u001b[37m \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m\u001b[37m \u001b[39;49;00msed\u001b[37m \u001b[39;49;00m-i\u001b[37m \u001b[39;49;00m\u001b[33m's|/v1/completions|/invocations|g'\u001b[39;49;00m\u001b[37m \u001b[39;49;00mvllm/entrypoints/openai/api_server.py\u001b[37m \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mCOPY\u001b[39;49;00m\u001b[37m \u001b[39;49;00msagemaker_entrypoint.sh\u001b[37m \u001b[39;49;00mentrypoint.sh\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m\u001b[37m \u001b[39;49;00mchmod\u001b[37m \u001b[39;49;00m+x\u001b[37m \u001b[39;49;00mentrypoint.sh\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m\u001b[37m \u001b[39;49;00m[\u001b[33m\"./entrypoint.sh\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "End of Dockerfile ----------------"
     ]
    }
   ],
   "source": [
    "!pygmentize sagemaker_vllm/Dockerfile\n",
    "!printf \"\\n\\n\\nEnd of Dockerfile ----------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e77e4f-5a19-476a-a0a8-d93141a174f3",
   "metadata": {},
   "source": [
    "In the output of the above cell, you can see that we:\n",
    "* fix to a specific vLLM container version, which we pass when we build the image\n",
    "* replace the `/health` and `/v1/completions` server routes by `/ping` and `/invocations` in the main server launch script, as required by SageMaker\n",
    "* copy our entrypoint script to the container, and set it as the ENTRYPOINT command, as required by SageMaker\n",
    "\n",
    "! NOTE !: if you change the vLLM base container version, check to make sure the string replacements above still work as intended, and the path to the main server launch script still holds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49115179-25f8-494b-b408-4f85195cd0b6",
   "metadata": {},
   "source": [
    "## Activating Docker for Jupyterlab in Sagemaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981057a2-e7f9-475a-aafc-c6dc6d1adb4d",
   "metadata": {},
   "source": [
    "Make sure to install docker in Sagemaker Studio Jupyterlab. This can also be run in terminal (File - New - Terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d66c95-9b65-43f9-a1dc-15e3eb74da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "ca-certificates is already the newest version (20230311ubuntu0.22.04.1).\n",
      "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
      "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpg: cannot open '/dev/tty': No such device or address\n",
      "curl: (23) Failure writing output to destination\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:3 https://download.docker.com/linux/ubuntu jammy InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "docker-ce-cli is already the newest version (5:20.10.24~3-0~ubuntu-jammy).\n",
      "docker-compose-plugin is already the newest version (2.28.1-1~ubuntu.22.04~jammy).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
      "Client: Docker Engine - Community\n",
      " Version:           20.10.24\n",
      " API version:       1.41\n",
      " Go version:        go1.19.7\n",
      " Git commit:        297e128\n",
      " Built:             Tue Apr  4 18:21:03 2023\n",
      " OS/Arch:           linux/amd64\n",
      " Context:           default\n",
      " Experimental:      true\n",
      "\n",
      "Server:\n",
      " Engine:\n",
      "  Version:          20.10.25\n",
      "  API version:      1.41 (minimum version 1.12)\n",
      "  Go version:       go1.20.10\n",
      "  Git commit:       5df983c\n",
      "  Built:            Fri Oct 13 22:46:59 2023\n",
      "  OS/Arch:          linux/amd64\n",
      "  Experimental:     false\n",
      " containerd:\n",
      "  Version:          1.7.11\n",
      "  GitCommit:        64b8a811b07ba6288238eefc14d898ee0b5b99ba\n",
      " runc:\n",
      "  Version:          1.1.11\n",
      "  GitCommit:        4bccb38cc9cf198d52bebf2b3a90cd14e7af8c06\n",
      " docker-init:\n",
      "  Version:          0.19.0\n",
      "  GitCommit:        de40ad0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./setup_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0c937-9e17-42ea-afc5-31d5dbff6331",
   "metadata": {},
   "source": [
    "We are good to go! We build the new container image and push it to a new ECR repository. Note SageMaker [supports private Docker registries](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02779726-b57f-472b-9d09-6a1aabc56a10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/8 : ARG VERSION\n",
      "Step 2/8 : FROM vllm/vllm-openai:$VERSION\n",
      "v0.3.3: Pulling from vllm/vllm-openai\n",
      "aece8493d397: Pulling fs layer\n",
      "45f7ea5367fe: Pulling fs layer\n",
      "3d97a47c3c73: Pulling fs layer\n",
      "12cd4d19752f: Pulling fs layer\n",
      "da5a484f9d74: Pulling fs layer\n",
      "5e5846364eee: Pulling fs layer\n",
      "fd355de1d1f2: Pulling fs layer\n",
      "3480bb79c638: Pulling fs layer\n",
      "e7016935dd60: Pulling fs layer\n",
      "0e5efb84a642: Pulling fs layer\n",
      "089120e96587: Pulling fs layer\n",
      "d0eb2e2560f1: Pulling fs layer\n",
      "f57296e87535: Pulling fs layer\n",
      "19a1b014b8cd: Pulling fs layer\n",
      "a4618d76678f: Pulling fs layer\n",
      "16b49417b04a: Pulling fs layer\n",
      "e7016935dd60: Waiting\n",
      "0e5efb84a642: Waiting\n",
      "089120e96587: Waiting\n",
      "d0eb2e2560f1: Waiting\n",
      "f57296e87535: Waiting\n",
      "19a1b014b8cd: Waiting\n",
      "a4618d76678f: Waiting\n",
      "16b49417b04a: Waiting\n",
      "12cd4d19752f: Waiting\n",
      "5e5846364eee: Waiting\n",
      "fd355de1d1f2: Waiting\n",
      "3480bb79c638: Waiting\n",
      "da5a484f9d74: Waiting\n",
      "aece8493d397: Verifying Checksum\n",
      "aece8493d397: Download complete\n",
      "45f7ea5367fe: Verifying Checksum\n",
      "45f7ea5367fe: Download complete\n",
      "12cd4d19752f: Verifying Checksum\n",
      "12cd4d19752f: Download complete\n",
      "da5a484f9d74: Verifying Checksum\n",
      "da5a484f9d74: Download complete\n",
      "fd355de1d1f2: Verifying Checksum\n",
      "fd355de1d1f2: Download complete\n",
      "3480bb79c638: Verifying Checksum\n",
      "3480bb79c638: Download complete\n",
      "e7016935dd60: Verifying Checksum\n",
      "e7016935dd60: Download complete\n",
      "3d97a47c3c73: Verifying Checksum\n",
      "3d97a47c3c73: Download complete\n",
      "089120e96587: Verifying Checksum\n",
      "089120e96587: Download complete\n",
      "d0eb2e2560f1: Verifying Checksum\n",
      "d0eb2e2560f1: Download complete\n",
      "0e5efb84a642: Verifying Checksum\n",
      "0e5efb84a642: Download complete\n",
      "19a1b014b8cd: Verifying Checksum\n",
      "19a1b014b8cd: Download complete\n",
      "a4618d76678f: Verifying Checksum\n",
      "a4618d76678f: Download complete\n",
      "16b49417b04a: Verifying Checksum\n",
      "16b49417b04a: Download complete\n",
      "aece8493d397: Pull complete\n",
      "45f7ea5367fe: Pull complete\n",
      "3d97a47c3c73: Pull complete\n",
      "12cd4d19752f: Pull complete\n",
      "da5a484f9d74: Pull complete\n",
      "5e5846364eee: Download complete\n",
      "5e5846364eee: Pull complete\n",
      "fd355de1d1f2: Pull complete\n",
      "f57296e87535: Verifying Checksum\n",
      "f57296e87535: Download complete\n",
      "3480bb79c638: Pull complete\n",
      "e7016935dd60: Pull complete\n",
      "0e5efb84a642: Pull complete\n",
      "089120e96587: Pull complete\n",
      "d0eb2e2560f1: Pull complete\n",
      "f57296e87535: Pull complete\n",
      "19a1b014b8cd: Pull complete\n",
      "a4618d76678f: Pull complete\n",
      "16b49417b04a: Pull complete\n",
      "Digest: sha256:4aea20de3b421f7775cfdc6468a04a29d0fcfc3603ad3b18aab4ef1f4652769d\n",
      "Status: Downloaded newer image for vllm/vllm-openai:v0.3.3\n",
      " ---> 3b8966176bfc\n",
      "Step 3/8 : RUN sed -i 's|/health|/ping|g' vllm/entrypoints/openai/api_server.py\n",
      " ---> Running in bc33d44051be\n",
      "Removing intermediate container bc33d44051be\n",
      " ---> e8d28d4b5717\n",
      "Step 4/8 : RUN sed -i 's|/v1/completions|/invocations|g' vllm/entrypoints/openai/api_server.py\n",
      " ---> Running in a134aa4aee29\n",
      "Removing intermediate container a134aa4aee29\n",
      " ---> d19755d10c27\n",
      "Step 5/8 : COPY sagemaker_entrypoint.sh entrypoint.sh\n",
      " ---> 33b9d230d1f7\n",
      "Step 6/8 : RUN chmod +x entrypoint.sh\n",
      " ---> Running in 6e534d0074ce\n",
      "Removing intermediate container 6e534d0074ce\n",
      " ---> 7f82f53afb98\n",
      "Step 7/8 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Running in 314ef45ec38c\n",
      "Removing intermediate container 314ef45ec38c\n",
      " ---> 18ac4a0276ff\n",
      "Step 8/8 : LABEL com.amazon.studio.user.resources=true\n",
      " ---> Running in b2fceab9dfd2\n",
      "Removing intermediate container b2fceab9dfd2\n",
      " ---> b8f551a5f858\n",
      "Successfully built b8f551a5f858\n",
      "Successfully tagged sagemaker-vllm:v0.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/sagemaker-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [626723862963.dkr.ecr.us-east-1.amazonaws.com/sagemaker-vllm]\n",
      "ec691392a2a4: Preparing\n",
      "359b0c42c430: Preparing\n",
      "a39435363212: Preparing\n",
      "e7e638fe62c8: Preparing\n",
      "12947d07ed5f: Preparing\n",
      "067e9baa9a0d: Preparing\n",
      "214b6ff61148: Preparing\n",
      "0fd53473730f: Preparing\n",
      "ca336086e060: Preparing\n",
      "c501b4875b93: Preparing\n",
      "674396d66abf: Preparing\n",
      "600c676771a0: Preparing\n",
      "6ac15100dff6: Preparing\n",
      "40f0eb1871b9: Preparing\n",
      "8d113b7b997c: Preparing\n",
      "cd77f58b80cd: Preparing\n",
      "e4b1bddcbe63: Preparing\n",
      "765423415d69: Preparing\n",
      "7b9433fba79b: Preparing\n",
      "256d88da4185: Preparing\n",
      "674396d66abf: Waiting\n",
      "600c676771a0: Waiting\n",
      "6ac15100dff6: Waiting\n",
      "40f0eb1871b9: Waiting\n",
      "8d113b7b997c: Waiting\n",
      "cd77f58b80cd: Waiting\n",
      "e4b1bddcbe63: Waiting\n",
      "765423415d69: Waiting\n",
      "7b9433fba79b: Waiting\n",
      "256d88da4185: Waiting\n",
      "067e9baa9a0d: Waiting\n",
      "214b6ff61148: Waiting\n",
      "0fd53473730f: Waiting\n",
      "c501b4875b93: Waiting\n",
      "ca336086e060: Waiting\n",
      "12947d07ed5f: Layer already exists\n",
      "067e9baa9a0d: Layer already exists\n",
      "214b6ff61148: Layer already exists\n",
      "0fd53473730f: Layer already exists\n",
      "ca336086e060: Layer already exists\n",
      "a39435363212: Pushed\n",
      "ec691392a2a4: Pushed\n",
      "c501b4875b93: Layer already exists\n",
      "e7e638fe62c8: Pushed\n",
      "359b0c42c430: Pushed\n",
      "674396d66abf: Layer already exists\n",
      "600c676771a0: Layer already exists\n",
      "40f0eb1871b9: Layer already exists\n",
      "6ac15100dff6: Layer already exists\n",
      "8d113b7b997c: Layer already exists\n",
      "cd77f58b80cd: Layer already exists\n",
      "e4b1bddcbe63: Layer already exists\n",
      "765423415d69: Layer already exists\n",
      "7b9433fba79b: Layer already exists\n",
      "256d88da4185: Pushed\n",
      "v0.3.3: digest: sha256:12a647b111f54638ae37c8fa9c7f06219c926506bf44ad376f6494941a22278c size: 4509\n"
     ]
    }
   ],
   "source": [
    "%%bash -s {region}\n",
    "algorithm_name=\"sagemaker-vllm\"  # name of your algorithm\n",
    "tag=\"v0.3.3\"\n",
    "region=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "image_uri=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" --region $region > /dev/null\n",
    "fi\n",
    "\n",
    "cd sagemaker_vllm/ && docker build --network=sagemaker --build-arg VERSION=$tag -t ${algorithm_name}:${tag} .\n",
    "\n",
    "# Authenticate Docker to an Amazon ECR registry\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Tag the image\n",
    "docker tag ${algorithm_name}:${tag} ${image_uri}\n",
    "\n",
    "# Push the image to the repository\n",
    "docker push ${image_uri}\n",
    "\n",
    "# Save image name to tmp file to use when deploying endpoint\n",
    "echo $image_uri > /tmp/image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d38628-9317-408f-906d-e5c9ce80fbb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"download_adapter\"></a>\n",
    "## Download adapter from HuggingFace Hub and push it to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6cb518-220c-491f-ae30-50f14f2b4ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are going to simulate storing our adapter weights on S3, and having SageMaker download them upfront when we provision the endpoint. This enables most scenarios, including deployment after you’ve finetuned your own adapters and pushed them to S3, as well as securing deployments with no internet access inside your VPC, as detailed in this [blog post](https://www.philschmid.de/sagemaker-llm-vpc#2-upload-the-model-to-amazon-s3).\n",
    "\n",
    "We first download an adapter trained with Mistral Instruct v0.1 as the base model to a local directory. This particular adapter was trained on GSM8K, a grade school math dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3230c3b-7740-4727-b8a1-eda109afe077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6719189994ae4f5ab91f6633d9bb4687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/notebooks/sagemaker/03_multi_adapter_hosting_sagemaker_vllm/mistral-adapter'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n",
    "# create model dir\n",
    "model_dir = Path('mistral-adapter')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(\n",
    "    HF_MODEL_ID,\n",
    "    local_dir=str(model_dir), # download to model dir\n",
    "    local_dir_use_symlinks=False, # use no symlinks to save disk space\n",
    "    revision=\"main\", # use a specific revision, e.g. refs/pr/21\n",
    "    cache_dir='/home/ec2-user/SageMaker/.cache/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b00f3-ca40-4b87-9456-71df3e6efe95",
   "metadata": {},
   "source": [
    "We copy this same adapter `n_adapters` times to different S3 prefixes in our SageMaker session bucket, simulating a large number of adapters we want to serve on the same endpoint and underlying GPU. We name the last prefix directory (leaf directory) as integer indexes, but you can change this to reflect the task or name of each adapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9a490c-a540-4e0d-8a44-a8b886bbfac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/1\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/2\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/3\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/4\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/5\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/6\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/7\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/8\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/9\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/10\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/11\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/12\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/13\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/14\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/15\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/16\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/17\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/18\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/19\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/20\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/21\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/22\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/23\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/24\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/25\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/26\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/27\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/28\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/29\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/30\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/31\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/32\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/33\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/34\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/35\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/36\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/37\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/38\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/39\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/40\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/41\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/42\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/43\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/44\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/45\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/46\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/47\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/48\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/49\n",
      "Uploaded folder to S3: sagemaker-us-east-1-626723862963/vllm/mistral-adapters/50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_folder_to_s3(local_path, s3_bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_object_key = os.path.join(s3_prefix, os.path.relpath(local_file_path, local_path))\n",
    "            s3.upload_file(local_file_path, s3_bucket, s3_object_key)\n",
    "\n",
    "# Upload the folder n_adapters times under different prefixes\n",
    "n_adapters=50\n",
    "base_prefix = 'vllm/mistral-adapters'\n",
    "for i in range(1, n_adapters+1):\n",
    "    prefix = f'{base_prefix}/{i}'\n",
    "    upload_folder_to_s3(model_dir, sagemaker_session_bucket, prefix)\n",
    "    print(f'Uploaded folder to S3: {sagemaker_session_bucket}/{prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aadcf5-dc32-4bc8-955b-16d4011a19ab",
   "metadata": {},
   "source": [
    "<a id=\"manifest\"></a>\n",
    "## Build LoRA modules manifest file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e454773-7a38-41cc-bb84-547550f6ee78",
   "metadata": {},
   "source": [
    "Now, we build the manifest file. The `--lora-modules` arg syntax to follow is `MODULE_NAME_1=LOCAL_DIRECTORY_1 MODULE_NAME_2=LOCAL_DIRECTORY_2 ...`. \n",
    "\n",
    "To maintain consistency and ease of management, we want our module names (which we will invoke later) to match the names of the leaf directories on S3 that hold each adapter artifact. In order to do this, we list the contents of our `base_prefix` directory on S3, parse the leaf directory names, and use them to build the `--lora-modules` argument. In this case, the leaf names will be integers in the range of 1 to `n_adapters`; however, the following cells can be reused for any S3 prefix that holds directories with LoRA artifacts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c8d04b0-63fa-4424-a3a4-177688c16023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "def list_s3_leaf_directories(bucket_name, prefix):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "\n",
    "    leaf_directories = []\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        directory_path = prefix['Prefix']\n",
    "        directory_name = directory_path.rstrip('/').rsplit('/', 1)[-1]  # Extract the last part of the path, the leaf directory\n",
    "        leaf_directories.append(directory_name)\n",
    "\n",
    "    return leaf_directories\n",
    "\n",
    "\n",
    "bucket_name = sagemaker_session_bucket\n",
    "prefix = base_prefix+'/'\n",
    "leaf_directories = list_s3_leaf_directories(bucket_name, prefix)\n",
    "print(leaf_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913ed58-a7db-4fc1-8bd8-3f6da1767d75",
   "metadata": {},
   "source": [
    "Now, we build the manifest file and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d8a9cc-cff9-4229-96e2-c6856b582370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_modules_manifest_file = \"lora_modules.txt\"\n",
    "\n",
    "with open(lora_modules_manifest_file, \"w\") as file:\n",
    "    vllm_arg_pattern = \" \".join([f\"{leaf_dir}=/opt/ml/model/{leaf_dir}/\" for leaf_dir in leaf_directories])\n",
    "    file.write(vllm_arg_pattern)\n",
    "\n",
    "# Upload to S3\n",
    "s3.upload_file(lora_modules_manifest_file, sagemaker_session_bucket, f'{base_prefix}/{lora_modules_manifest_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383850b2-6e87-4157-be5e-bf4b52670a25",
   "metadata": {},
   "source": [
    "We can verify the file was uploaded to the correct directory (our base_prefix) on the following cell. If there is no output, the file is not where it should be, and you should verify the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83086429-632b-40b4-aa33-20e24bd3ca99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:23:48       1031 lora_modules.txt\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls \"s3://$sagemaker_session_bucket/$base_prefix/\" | grep \"$lora_modules_manifest_file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ed1b2-8b7c-4e9b-bace-7ace5f78e1ca",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## Deploy SageMaker endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24eae-766a-4ec7-8ecf-de45f1092051",
   "metadata": {},
   "source": [
    "Now we deploy a SageMaker endpoint, pointing to our `base_prefix` as the `model_data` parameter.\n",
    "\n",
    "Let's dissect what is happening here:\n",
    "* as explained in the SageMaker [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts), SageMaker downloads model artifacts under the provided `S3URI` to the `/opt/ml/model` directory; your container has read-only access to this directory\n",
    "* by specifying that our data is in an `S3Prefix` and `CompressionType` is `None`, you do not need to tar.gz the `base_prefix` directory; SageMaker will download all the files and directories in our `base_prefix` in uncompressed format, replicating the S3 directory structure (i.e. the `base_prefix` dir structure will match the `/opt/ml/model` dir structure). This is why we pass `/opt/ml/model` as the `MODEL_DIR` in the next cell, and why we placed the lora manifest file in the root of the `base_prefix` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9fb4192-ddab-483b-92ef-125216270763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440ab125acd24826b8efb1d47141fd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "from pathlib import Path\n",
    "hf_token = Path(\"/home/sagemaker-user/.cache/huggingface/token\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02925d2b-147c-4c84-af40-cc7c32fa7993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint sm-vllm-2024-07-05-12-23-48-834: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create SM Model, pass in model data as a whole prefix of uncompressed model artifacts\u001b[39;00m\n\u001b[1;32m     31\u001b[0m vllm_model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m     32\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39mimage_uri[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     33\u001b[0m     model_data\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     40\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m vllm_predictor \u001b[38;5;241m=\u001b[39m \u001b[43mvllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealth_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mJSONSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mJSONDeserializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:1721\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, **kwargs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1719\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1734\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5711\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5708\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   5709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 5711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5717\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:4569\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5354\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   5349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   5350\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5351\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5352\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5353\u001b[0m         )\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   5355\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5356\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5357\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5358\u001b[0m     )\n\u001b[1;32m   5359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint sm-vllm-2024-07-05-12-23-48-834: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "from sagemaker import Model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Retrieve image_uri from tmp file\n",
    "image_uri = !cat /tmp/image_uri\n",
    "# Increased health check timeout to give time for model download\n",
    "health_check_timeout = 800\n",
    "# Endpoint configs\n",
    "number_of_gpu = 1\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sm-vllm\")\n",
    "\n",
    "# Env vars required for server launch\n",
    "config = {\n",
    "  'MODEL_DIR': '/opt/ml/model', # root dir for adatper dirs and manifest file\n",
    "  'LORA_MODULES_MANIFEST_FILE': lora_modules_manifest_file, # manifest file name\n",
    "  'HF_MODEL_ID': \"mistralai/Mistral-7B-Instruct-v0.1\", # model_id from hf.co/models\n",
    "  'HUGGING_FACE_HUB_TOKEN': hf_token,\n",
    "  'MAX_MODEL_LEN': json.dumps(4096),  # max length of input text\n",
    "  'MAX_GPU_LORAS': json.dumps(20), # max number of adapters usable in single batch\n",
    "  'MAX_CPU_LORAS': json.dumps(50), # max number of adapters that can be held in CPU mem\n",
    "  'MAX_NUM_SEQS': json.dumps(100), # max number of sequences per iteration\n",
    "  'ENFORCE_EAGER': json.dumps(False), # whether to turn off CUDA graphs and enforce eager mode (saves GPU mem) \n",
    "}\n",
    "\n",
    "# Create SM Model, pass in model data as a whole prefix of uncompressed model artifacts\n",
    "vllm_model = Model(\n",
    "    image_uri=image_uri[0],\n",
    "    model_data={\n",
    "        'S3DataSource':{\n",
    "            'S3Uri': f's3://{sagemaker_session_bucket}/{base_prefix}/',\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'CompressionType': 'None'}},\n",
    "    env=config,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "vllm_predictor = vllm_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c9e7a-8a1e-4b02-bc60-77f87f7aa5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can reinstantiate the Predictor object if you restart the notebook or Predictor is None\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "endpoint_name = endpoint_name\n",
    "\n",
    "vllm_predictor = Predictor(\n",
    "    endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308c12e-365c-4737-90f5-998f849beffe",
   "metadata": {},
   "source": [
    "<a id=\"compare\"></a>\n",
    "## Invoke base model and adapter, compare outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ae3e2-8634-4e4d-957a-a3d81d7490e7",
   "metadata": {},
   "source": [
    "We can invoke the base Mistral model, as well as any of the adapters downloaded to our endpoint! vLLM will take care of downloading them, continuously batch requests for different adapters, and manage DRAM and RAM by loading/offloading adapters.\n",
    "\n",
    "Let’s inspect the difference between the base model’s response and the adapter’s response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167babae-0fb9-4a7f-aba9-fe4e266ebdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "\n",
    "payload_base = {\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"temperature\":0\n",
    "}\n",
    "\n",
    "\n",
    "payload_adapter = {\n",
    "    \"model\": \"1\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"temperature\":0\n",
    "}\n",
    "\n",
    "response_base = vllm_predictor.predict(payload_base)\n",
    "response_adapter = vllm_predictor.predict(payload_adapter)\n",
    "\n",
    "print(f'Base model output:\\n-------------\\n {response_base[\"choices\"][0][\"text\"]}')\n",
    "print(f'\\nAdapter output:\\n-------------\\n {response_adapter[\"choices\"][0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd03354-75e8-47e9-91e8-dbf52d95d629",
   "metadata": {},
   "source": [
    "You can also check out the full details of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45874488-5ffb-41bb-9bd5-d7f7343928a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1010056-9b46-4c62-880c-9f4552aa2af7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## Benchmark single adapter vs. random access to adapters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67667c2d-c154-42dc-9be7-d577f8855e1d",
   "metadata": {},
   "source": [
    "First, we individually call each of the adapters in sequence, to make sure they are previously loaded to either GPU or CPU memory. We want to exclude disk read latency from the benchmark metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeea08-31e8-411c-a57b-c4acdd702f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(1,n_adapters+1)):\n",
    "    payload_adapter = {\n",
    "        \"model\": str(i),\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 64,\n",
    "        \"temperature\":0\n",
    "    }\n",
    "    vllm_predictor.predict(payload_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eeb399-7490-4697-9871-777038232200",
   "metadata": {},
   "source": [
    "Now we are ready to benchmark. For the single adapter case, we invoke the adapter `total_requests` times from `num_threads` concurrent clients.\n",
    "\n",
    "For the multi-adapter case, we invoke a random adapter from any of the clients, until all adapters have been invoked `total_requests//num_adapters` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520c962-f064-4e8a-a9ce-2405084e3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust if you run into connection pool errors\n",
    "# import botocore\n",
    "\n",
    "# Configure botocore to use a larger connection pool\n",
    "# config = botocore.config.Config(max_pool_connections=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94ddaa-4d57-4139-8e76-b87593fb155e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Configuration\n",
    "total_requests = 300\n",
    "num_adapters = 50\n",
    "num_threads = 20  # Adjust based on your system capabilities\n",
    "\n",
    "\n",
    "# Shared lock and counters for # invocations of each adapter \n",
    "adapter_counters = [total_requests // num_adapters] * num_adapters\n",
    "counters_lock = threading.Lock()\n",
    "\n",
    "def invoke_adapter(aggregate_latency, single_adapter=False):\n",
    "    global total_requests\n",
    "    latencies = []\n",
    "    while True:\n",
    "        with counters_lock:\n",
    "            if single_adapter:\n",
    "                adapter_id = 1\n",
    "                if total_requests > 0:\n",
    "                    total_requests -= 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                # Find an adapter that still needs to be called\n",
    "                remaining_adapters = [i for i, count in enumerate(adapter_counters) if count > 0]\n",
    "                if not remaining_adapters:\n",
    "                    break\n",
    "                adapter_id = random.choice(remaining_adapters) + 1\n",
    "                adapter_counters[adapter_id - 1] -= 1\n",
    "\n",
    "        prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "\n",
    "        payload_adapter = {\n",
    "            \"model\": str(adapter_id),\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 64,\n",
    "            \"temperature\":0\n",
    "        }\n",
    "        start_time = time.time()\n",
    "        response_adapter = vllm_predictor.predict(payload_adapter)\n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    aggregate_latency.extend(latencies)\n",
    "\n",
    "def benchmark_scenario(single_adapter=False):\n",
    "    threads = []\n",
    "    all_latencies = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(num_threads):\n",
    "        thread_latencies = []\n",
    "        all_latencies.append(thread_latencies)\n",
    "        thread = threading.Thread(target=invoke_adapter, args=(thread_latencies, single_adapter))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    total_latency = sum([sum(latencies) for latencies in all_latencies])\n",
    "    total_requests_made = sum([len(latencies) for latencies in all_latencies])\n",
    "    average_latency = total_latency / total_requests_made\n",
    "    throughput = total_requests_made / (time.time() - start_time)\n",
    "\n",
    "    print(f\"Total Time: {time.time() - start_time}s\")\n",
    "    print(f\"Average Latency: {average_latency}s\")\n",
    "    print(f\"Throughput: {throughput} requests/s\")\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Benchmarking: Single Adapter Multiple Times\")\n",
    "benchmark_scenario(single_adapter=True)\n",
    "\n",
    "print(\"\\nBenchmarking: Multiple Adapters with Random Access\")\n",
    "benchmark_scenario()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9521-f5ed-487b-ba84-5a834add3d78",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "## Cleanup endpoint resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920da7-c0ce-4f3f-9eea-297abb49b342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vllm_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
