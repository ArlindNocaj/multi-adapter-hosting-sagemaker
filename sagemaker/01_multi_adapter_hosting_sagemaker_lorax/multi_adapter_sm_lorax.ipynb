{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844f22ae-6b01-4a17-820b-83f9d34de23e",
   "metadata": {},
   "source": [
    "# Serve multiple LoRA adapters efficiently on SageMaker w/ LoRAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cd38c-ad40-4a3e-b225-4d1f707a0c0e",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to serve many Low-Rank Adapters (LoRA) on top of the same base model efficiently on the same GPU. In order to do this, we'll deploy the LoRA Exchange ([LoRAX](https://github.com/predibase/lorax/tree/main)) inference server to SageMaker Hosting. \n",
    "\n",
    "These are the steps we will take:\n",
    "\n",
    "1. [Setup our environment](#setup)\n",
    "2. [Build a new LoRAX container image compatible with SageMaker, push it to Amazon ECR](#container)\n",
    "3. [Download adapters from the HuggingFace Hub and upload them to S3](#download_adapter)\n",
    "4. [Deploy the extended LoRAX container to SageMaker](#deploy)\n",
    "5. [Compare outputs of the base model and the adapter model](#compare)\n",
    "6. [Benchmark our deployed endpoint under different traffic patterns - same adapter, and random access to many adapters](#benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65e8be-3073-4436-b311-11a3de09e101",
   "metadata": {},
   "source": [
    "## What is LoRAX? \n",
    "\n",
    "LoRAX is a production-ready framework specialized in multi-adapter serving that  efficiently share the same GPU resources, which dramatically reduces the cost of serving without compromising on throughput or latency. Some of the features that enable this are: \n",
    "\n",
    "* Dynamic Adapter Loading - fine-tuned LoRA weights are loaded from storage (local or remote) just-in-time as requests come in at runtime\n",
    "* Tiered Weight Caching - fast exchanging of LoRA adapters between requests, and offloading of adapter weights to CPU and disk as they are not needed to avoid out-of-memory errors.\n",
    "* Continuous Multi-Adapter Batching - a fair scheduling policy that continuously batches requests targeted at different LoRA adapters so they can be processed in paralle, optimizing aggregate throughput.\n",
    "* Optimized Inference - high throughput and low latency optimizations including tensor parallelism, pre-compiled CUDA kernels ([flash-attention](https://arxiv.org/abs/2307.08691), [paged attention](https://arxiv.org/abs/2309.06180), [SGMV](https://arxiv.org/abs/2310.18547)), quantization, token streaming.\n",
    "\n",
    "You can read more about LoRAX [here](https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4372522-fb5d-4cad-be37-d4f21f794698",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdf3acc-e2fc-48ca-a21e-4377c7638d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.2 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.34.139 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U boto3 sagemaker huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d8394b-5d69-4c02-97fe-d00eccc4da26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker default S3 bucket: sagemaker-us-east-1-626723862963\n",
      "sagemaker role arn: arn:aws:iam::626723862963:role/service-role/AmazonSageMaker-ExecutionRole-20231214T145077\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker default S3 bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eddcf1-0d92-4bcd-b644-4a0ca75d289c",
   "metadata": {},
   "source": [
    "## Activating Docker in Jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de5a1a-3fb1-4ca3-9054-c44de16fac3b",
   "metadata": {},
   "source": [
    "Make sure to install docker in Sagemaker Studio Jupyterlab. This can also be run in terminal (File - New - Terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94b268b3-3e4e-4acb-808c-5acb19cc06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "ca-certificates is already the newest version (20230311ubuntu0.22.04.1).\n",
      "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
      "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpg: cannot open '/dev/tty': No such device or address\n",
      "curl: (23) Failure writing output to destination\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://download.docker.com/linux/ubuntu jammy InRelease\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Fetched 257 kB in 1s (414 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "docker-ce-cli is already the newest version (5:20.10.24~3-0~ubuntu-jammy).\n",
      "docker-compose-plugin is already the newest version (2.28.1-1~ubuntu.22.04~jammy).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
      "Client: Docker Engine - Community\n",
      " Version:           20.10.24\n",
      " API version:       1.41\n",
      " Go version:        go1.19.7\n",
      " Git commit:        297e128\n",
      " Built:             Tue Apr  4 18:21:03 2023\n",
      " OS/Arch:           linux/amd64\n",
      " Context:           default\n",
      " Experimental:      true\n",
      "\n",
      "Server:\n",
      " Engine:\n",
      "  Version:          20.10.25\n",
      "  API version:      1.41 (minimum version 1.12)\n",
      "  Go version:       go1.20.10\n",
      "  Git commit:       5df983c\n",
      "  Built:            Fri Oct 13 22:46:59 2023\n",
      "  OS/Arch:          linux/amd64\n",
      "  Experimental:     false\n",
      " containerd:\n",
      "  Version:          1.7.11\n",
      "  GitCommit:        64b8a811b07ba6288238eefc14d898ee0b5b99ba\n",
      " runc:\n",
      "  Version:          1.1.11\n",
      "  GitCommit:        4bccb38cc9cf198d52bebf2b3a90cd14e7af8c06\n",
      " docker-init:\n",
      "  Version:          0.19.0\n",
      "  GitCommit:        de40ad0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash setup_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d6fd3-5d59-4bc2-8cf2-ad4c32d072b7",
   "metadata": {},
   "source": [
    "<a id=\"container\"></a>\n",
    "## Build a new LoRAX container image compatible with SageMaker, push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9d085-897f-46a4-875f-798f0ebb1f61",
   "metadata": {},
   "source": [
    "This example includes a `Dockerfile` and `sagemaker_entrypoint.sh` in the `sagemaker_lorax` directory. Building this new container image makes LoRAX compatible with SageMaker Hosting, namely launching the server on port 8080 via the container's `ENTRYPOINT` instruction. [Here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) you can find the basic interfaces required to adapt any container for deployment on Sagemaker Hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd0eafe8-2398-43e0-9dd6-e211c23ba5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "if [[ -z \"${HF_MODEL_ID}\" ]]; then\n",
      "  echo \"HF_MODEL_ID must be set\"\n",
      "  exit 1\n",
      "fi\n",
      "export MODEL_ID=\"${HF_MODEL_ID}\"\n",
      "\n",
      "if [[ -n \"${HF_MODEL_REVISION}\" ]]; then\n",
      "  export REVISION=\"${HF_MODEL_REVISION}\"\n",
      "fi\n",
      "\n",
      "if [[ -n \"${SM_NUM_GPUS}\" ]]; then\n",
      "  export NUM_SHARD=\"${SM_NUM_GPUS}\"\n",
      "fi\n",
      "\n",
      "if [[ -n \"${HF_MODEL_QUANTIZE}\" ]]; then\n",
      "  export QUANTIZE=\"${HF_MODEL_QUANTIZE}\"\n",
      "fi\n",
      "\n",
      "if [[ -n \"${HF_MODEL_TRUST_REMOTE_CODE}\" ]]; then\n",
      "  export TRUST_REMOTE_CODE=\"${HF_MODEL_TRUST_REMOTE_CODE}\"\n",
      "fi\n",
      "\n",
      "if [[ -z \"${ADAPTER_BUCKET}\" ]]; then\n",
      "  echo \"Warning: ADAPTER_BUCKET not set. Only able to load local or HuggingFace Hub models.\"\n",
      "else\n",
      "  export PREDIBASE_MODEL_BUCKET=\"${ADAPTER_BUCKET}\"\n",
      "fi\n",
      "\n",
      "lorax-launcher --port 8080\n"
     ]
    }
   ],
   "source": [
    "!cat sagemaker_lorax/sagemaker_entrypoint.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f2cc01a-13e6-4d6e-9637-00dc2e94050d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG VERSION\n",
      "FROM ghcr.io/predibase/lorax:$VERSION\n",
      "\n",
      "COPY sagemaker_entrypoint.sh entrypoint.sh\n",
      "RUN chmod +x entrypoint.sh\n",
      "\n",
      "ENTRYPOINT [\"./entrypoint.sh\"]\n"
     ]
    }
   ],
   "source": [
    "!cat sagemaker_lorax/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0c937-9e17-42ea-afc5-31d5dbff6331",
   "metadata": {},
   "source": [
    "We build the new container image and push it to a new ECR repository. Note SageMaker [supports private Docker registries](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02779726-b57f-472b-9d09-6a1aabc56a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/6 : ARG VERSION\n",
      "Step 2/6 : FROM ghcr.io/predibase/lorax:$VERSION\n",
      " ---> 12cbb858e917\n",
      "Step 3/6 : COPY sagemaker_entrypoint.sh entrypoint.sh\n",
      " ---> Using cache\n",
      " ---> d022857f458a\n",
      "Step 4/6 : RUN chmod +x entrypoint.sh\n",
      " ---> Using cache\n",
      " ---> 5eeca0aab381\n",
      "Step 5/6 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Using cache\n",
      " ---> 6b18cfc543a7\n",
      "Step 6/6 : LABEL com.amazon.studio.user.resources=true\n",
      " ---> Using cache\n",
      " ---> 22f4f5d50a04\n",
      "Successfully built 22f4f5d50a04\n",
      "Successfully tagged sagemaker-lorax:0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/sagemaker-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [626723862963.dkr.ecr.us-east-1.amazonaws.com/sagemaker-lorax]\n",
      "03630b8fa156: Preparing\n",
      "cb150628e6cc: Preparing\n",
      "1d0b41ccda6f: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "6f8902c56d3a: Preparing\n",
      "8d781d80cb61: Preparing\n",
      "a789d8b3e5c9: Preparing\n",
      "4266187e4a78: Preparing\n",
      "1d8eb7c54d2b: Preparing\n",
      "f0ae60d5fc16: Preparing\n",
      "9ea406769079: Preparing\n",
      "dd129f32cbce: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "4bf8ae4ca4b4: Preparing\n",
      "dd2ba8e514ec: Preparing\n",
      "56de09d53165: Preparing\n",
      "0ed8bcd23002: Preparing\n",
      "3e9d873bbf45: Preparing\n",
      "4de6133ecfe4: Preparing\n",
      "c6ea34fb28c0: Preparing\n",
      "ea2f174ab3cb: Preparing\n",
      "0c395af0c0dc: Preparing\n",
      "62ef9850694f: Preparing\n",
      "3eab3f5945eb: Preparing\n",
      "a80e63203a09: Preparing\n",
      "e629e1f35af9: Preparing\n",
      "75b355150569: Preparing\n",
      "06a3bfc021d0: Preparing\n",
      "4194045cdf23: Preparing\n",
      "2952ab0c67d1: Preparing\n",
      "b68332006944: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "498bbcc60d01: Preparing\n",
      "c0e21dcee623: Preparing\n",
      "d6b19a46b795: Preparing\n",
      "e6c05e83c163: Preparing\n",
      "256d88da4185: Preparing\n",
      "4266187e4a78: Waiting\n",
      "1d8eb7c54d2b: Waiting\n",
      "f0ae60d5fc16: Waiting\n",
      "9ea406769079: Waiting\n",
      "dd129f32cbce: Waiting\n",
      "4bf8ae4ca4b4: Waiting\n",
      "dd2ba8e514ec: Waiting\n",
      "56de09d53165: Waiting\n",
      "0ed8bcd23002: Waiting\n",
      "3e9d873bbf45: Waiting\n",
      "4de6133ecfe4: Waiting\n",
      "c6ea34fb28c0: Waiting\n",
      "ea2f174ab3cb: Waiting\n",
      "0c395af0c0dc: Waiting\n",
      "62ef9850694f: Waiting\n",
      "3eab3f5945eb: Waiting\n",
      "a80e63203a09: Waiting\n",
      "e629e1f35af9: Waiting\n",
      "75b355150569: Waiting\n",
      "06a3bfc021d0: Waiting\n",
      "4194045cdf23: Waiting\n",
      "2952ab0c67d1: Waiting\n",
      "b68332006944: Waiting\n",
      "498bbcc60d01: Waiting\n",
      "c0e21dcee623: Waiting\n",
      "d6b19a46b795: Waiting\n",
      "e6c05e83c163: Waiting\n",
      "256d88da4185: Waiting\n",
      "8d781d80cb61: Waiting\n",
      "a789d8b3e5c9: Waiting\n",
      "cb150628e6cc: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "6f8902c56d3a: Layer already exists\n",
      "1d0b41ccda6f: Layer already exists\n",
      "03630b8fa156: Layer already exists\n",
      "8d781d80cb61: Layer already exists\n",
      "a789d8b3e5c9: Layer already exists\n",
      "4266187e4a78: Layer already exists\n",
      "1d8eb7c54d2b: Layer already exists\n",
      "f0ae60d5fc16: Layer already exists\n",
      "9ea406769079: Layer already exists\n",
      "dd129f32cbce: Layer already exists\n",
      "4bf8ae4ca4b4: Layer already exists\n",
      "dd2ba8e514ec: Layer already exists\n",
      "0ed8bcd23002: Layer already exists\n",
      "3e9d873bbf45: Layer already exists\n",
      "4de6133ecfe4: Layer already exists\n",
      "c6ea34fb28c0: Layer already exists\n",
      "ea2f174ab3cb: Layer already exists\n",
      "56de09d53165: Layer already exists\n",
      "0c395af0c0dc: Layer already exists\n",
      "62ef9850694f: Layer already exists\n",
      "3eab3f5945eb: Layer already exists\n",
      "e629e1f35af9: Layer already exists\n",
      "a80e63203a09: Layer already exists\n",
      "75b355150569: Layer already exists\n",
      "06a3bfc021d0: Layer already exists\n",
      "4194045cdf23: Layer already exists\n",
      "2952ab0c67d1: Layer already exists\n",
      "b68332006944: Layer already exists\n",
      "498bbcc60d01: Layer already exists\n",
      "c0e21dcee623: Layer already exists\n",
      "d6b19a46b795: Layer already exists\n",
      "256d88da4185: Layer already exists\n",
      "e6c05e83c163: Layer already exists\n",
      "0.8.0: digest: sha256:a24ca9a7de8d6043e581fcabcd703839a823323ffc2bb55ddab1ee4e7336ae6c size: 8095\n"
     ]
    }
   ],
   "source": [
    "%%bash -s {region}\n",
    "algorithm_name=\"sagemaker-lorax\"  # name of your algorithm\n",
    "tag=\"0.8.0\"\n",
    "region=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "image_uri=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" --region $region > /dev/null\n",
    "fi\n",
    "\n",
    "cd sagemaker_lorax/ && docker build --network=sagemaker --build-arg VERSION=$tag -t ${algorithm_name}:${tag} .\n",
    "\n",
    "# Authenticate Docker to an Amazon ECR registry\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Tag the image\n",
    "docker tag ${algorithm_name}:${tag} ${image_uri}\n",
    "\n",
    "# Push the image to the repository\n",
    "docker push ${image_uri}\n",
    "\n",
    "# Save image name to tmp file to use when deploying endpoint\n",
    "echo $image_uri > /tmp/image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d38628-9317-408f-906d-e5c9ce80fbb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"download_adapter\"></a>\n",
    "## Download adapter from HuggingFace Hub and push it to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6cb518-220c-491f-ae30-50f14f2b4ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are going to simulate storing our adapter weights on S3, and having LoRAX load them dynamically as we invoke them. This enables most scenarios, including deployment after you’ve finetuned your own adapter and pushed it to S3, as well as securing deployments with no internet access inside your VPC, as detailed in this [blog post](https://www.philschmid.de/sagemaker-llm-vpc#2-upload-the-model-to-amazon-s3).\n",
    "\n",
    "We first download an adapter trained with Mistral Instruct v0.1 as the base model to a local directory. This particular adapter was trained on GSM8K, a grade school math dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3230c3b-7740-4727-b8a1-eda109afe077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff34d94fb094916961d298c6406a53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/notebooks/sagemaker/01_multi_adapter_hosting_sagemaker_lorax/mistral-adapter'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n",
    "# create model dir\n",
    "model_dir = Path('mistral-adapter')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(\n",
    "    HF_MODEL_ID,\n",
    "    local_dir=str(model_dir), # download to model dir\n",
    "    local_dir_use_symlinks=False, # use no symlinks to save disk space\n",
    "    revision=\"main\", # use a specific revision, e.g. refs/pr/21\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b00f3-ca40-4b87-9456-71df3e6efe95",
   "metadata": {},
   "source": [
    "We copy this same adapter `n_adapters` times to different S3 prefixes in our SageMaker session bucket, simulating a large number of adapters we want to serve on the same endpoint and underlying GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d9a490c-a540-4e0d-8a44-a8b886bbfac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/01, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/02, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/03, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/04, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/05, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/06, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/07, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/08, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/09, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/10, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/11, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/12, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/13, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/14, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/15, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/16, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/17, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/18, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/19, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/20, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/21, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/22, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/23, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/24, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/25, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/26, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/27, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/28, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/29, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/30, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/31, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/32, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/33, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/34, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/35, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/36, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/37, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/38, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/39, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/40, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/41, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/42, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/43, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/44, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/45, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/46, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/47, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/48, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/49, bucket: sagemaker-us-east-1-626723862963\n",
      "Uploaded folder to S3 with prefix: lorax/mistral-adapters/50, bucket: sagemaker-us-east-1-626723862963\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_folder_to_s3(local_path, s3_bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_object_key = os.path.join(s3_prefix, os.path.relpath(local_file_path, local_path))\n",
    "            s3.upload_file(local_file_path, s3_bucket, s3_object_key)\n",
    "\n",
    "# Upload the folder n_adapters times under different prefixes\n",
    "n_adapters=50\n",
    "base_prefix = 'lorax/mistral-adapters'\n",
    "for i in range(1, n_adapters+1):\n",
    "    prefix = f'{base_prefix}/0{i}' if i < 10 else f'{base_prefix}/{i}'\n",
    "    upload_folder_to_s3(model_dir, sagemaker_session_bucket, prefix)\n",
    "    print(f'Uploaded folder to S3 with prefix: {prefix}, bucket: {sagemaker_session_bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ed1b2-8b7c-4e9b-bace-7ace5f78e1ca",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## Deploy SageMaker endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24eae-766a-4ec7-8ecf-de45f1092051",
   "metadata": {},
   "source": [
    "Now we deploy a SageMaker endpoint, pointing to our SageMaker session bucket as the ADAPTER_BUCKET env variable, which enables downloading adapters from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faea444e-9f68-4a08-8137-d9225d9a529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e4f126-ebc8-4c9c-8646-f3f6b8e349fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3d1877d1b540398af57e09bc58bf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "from pathlib import Path\n",
    "hf_token = Path(\"/home/sagemaker-user/.cache/huggingface/token\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02925d2b-147c-4c84-af40-cc7c32fa7993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "from sagemaker import Model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Retrieve image_uri from tmp file\n",
    "image_uri = !cat /tmp/image_uri\n",
    "# Increased health check timeout to give time for model download\n",
    "health_check_timeout = 800\n",
    "number_of_gpu = 1\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sm-lorax\")\n",
    "\n",
    "# Model and Endpoint configuration parameters\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"mistralai/Mistral-7B-Instruct-v0.1\", # model_id from hf.co/models\n",
    "  'HUGGING_FACE_HUB_TOKEN': hf_token,\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'ADAPTER_BUCKET': sagemaker_session_bucket,\n",
    "}\n",
    "\n",
    "lorax_model = Model(\n",
    "    image_uri=image_uri[0],\n",
    "    role=role,\n",
    "    env=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781e037-6612-4792-82d4-e888d8521661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lorax_predictor = lorax_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout, \n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da823e3-d1ed-42db-b701-5c9ef5a7ff0e",
   "metadata": {},
   "source": [
    "## Start Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53c9e7a-8a1e-4b02-bc60-77f87f7aa5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can reinstantiate the Predictor object if you restart the notebook or Predictor is None\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "n_adapters=50\n",
    "base_prefix = 'lorax/mistral-adapters'\n",
    "endpoint_name = \"sm-lorax-2024-06-30-14-57-07-207\"\n",
    "\n",
    "\n",
    "lorax_predictor = Predictor(\n",
    "    endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308c12e-365c-4737-90f5-998f849beffe",
   "metadata": {},
   "source": [
    "<a id=\"compare\"></a>\n",
    "## Invoke base model and adapter, compare outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ae3e2-8634-4e4d-957a-a3d81d7490e7",
   "metadata": {},
   "source": [
    "We can invoke the base Mistral model, as well as any of the adapters in our bucket! LoRAX will take care of downloading them, continuously batch requests for different adapters, and manage DRAM and RAM by loading/offloading adapters.\n",
    "\n",
    "Let’s inspect the difference between the base model’s response and the adapter’s response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ea512-e0af-40c5-ba3e-1253eedfd011",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ I observed a weird error that I haven't debugged yet, where S3 download failed for adapters ID 1 through 5, but worked as expected for all other adapters. Something with the S3 prefix. Added 0 before adapter id if id < 10 as a workaround.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "167babae-0fb9-4a7f-aba9-fe4e266ebdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output:\n",
      "-------------\n",
      " Let's break down the problem:\n",
      "\n",
      "1. In April, Natalia sold clips to 48 of her friends.\n",
      "2. In May, she sold half as many clips as in April, which means she sold 48/2 = 24 clips in May.\n",
      "\n",
      "Adapter output:\n",
      "-------------\n",
      " Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "In total, Natalia sold 48 + 24 = <<48+24=72>>72 clips in April and May.\n",
      "#### 72\n"
     ]
    }
   ],
   "source": [
    "prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "\n",
    "payload_base = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "    }\n",
    "}\n",
    "\n",
    "payload_adapter = {\n",
    "\"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"adapter_id\": f'{base_prefix}/01',\n",
    "        \"adapter_source\": \"s3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response_base = lorax_predictor.predict(payload_base)\n",
    "response_adapter = lorax_predictor.predict(payload_adapter)\n",
    "\n",
    "print(f'Base model output:\\n-------------\\n {response_base[0][\"generated_text\"]}')\n",
    "print(f'Adapter output:\\n-------------\\n {response_adapter[0][\"generated_text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1010056-9b46-4c62-880c-9f4552aa2af7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## Benchmark single adapter vs. random access to adapters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67667c2d-c154-42dc-9be7-d577f8855e1d",
   "metadata": {},
   "source": [
    "First, we individually call each of the adapters in sequence, to make sure they are previously downloaded to the endpoint instance’s disk. We want to exclude S3 download latency from the benchmark metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cdeea08-31e8-411c-a57b-c4acdd702f40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(1,n_adapters+1)):\n",
    "    adapter_id = f'{base_prefix}/0{i}' if i < 10 else f'{base_prefix}/{i}'\n",
    "    payload_adapter = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"adapter_id\": adapter_id,\n",
    "        \"adapter_source\": \"s3\"\n",
    "        }\n",
    "    }\n",
    "    lorax_predictor.predict(payload_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eeb399-7490-4697-9871-777038232200",
   "metadata": {},
   "source": [
    "Now we are ready to benchmark. For the single adapter case, we invoke the adapter `total_requests` times from `num_threads` concurrent clients.\n",
    "\n",
    "For the multi-adapter case, we invoke a random adapter from any of the clients, until all adapters have been invoked `total_requests//num_adapters` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c520c962-f064-4e8a-a9ce-2405084e3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust if you run into connection pool errors\n",
    "# import botocore\n",
    "\n",
    "# Configure botocore to use a larger connection pool\n",
    "# config = botocore.config.Config(max_pool_connections=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c94ddaa-4d57-4139-8e76-b87593fb155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking: Single Adapter Multiple Times\n",
      "Total Time: 281.5893428325653s\n",
      "Average Latency: 2.8096318435668945s\n",
      "Throughput: 7.102541547400547 requests/s\n",
      "\n",
      "Benchmarking: Multiple Adapters with Random Access\n",
      "Total Time: 279.85398745536804s\n",
      "Average Latency: 2.7977559057474135s\n",
      "Throughput: 7.146583920425574 requests/s\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Configuration\n",
    "total_requests = 500\n",
    "num_adapters = 50\n",
    "num_threads = 20  # Adjust based on your system capabilities\n",
    "\n",
    "\n",
    "# Shared lock and counters for # invocations of each adapter \n",
    "adapter_counters = [total_requests // num_adapters] * num_adapters\n",
    "counters_lock = threading.Lock()\n",
    "\n",
    "def invoke_adapter(aggregate_latency, single_adapter=False):\n",
    "    global total_requests\n",
    "    latencies = []\n",
    "    while True:\n",
    "        with counters_lock:\n",
    "            if single_adapter:\n",
    "                adapter_id = 1\n",
    "                if total_requests > 0:\n",
    "                    total_requests -= 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                # Find an adapter that still needs to be called\n",
    "                remaining_adapters = [i for i, count in enumerate(adapter_counters) if count > 0]\n",
    "                if not remaining_adapters:\n",
    "                    break\n",
    "                adapter_id = random.choice(remaining_adapters) + 1\n",
    "                adapter_counters[adapter_id - 1] -= 1\n",
    "\n",
    "        prompt = '[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]'\n",
    "        invoke_adapter_id = f'{base_prefix}/0{i}' if i < 10 else f'{base_prefix}/{i}'\n",
    "        payload_adapter = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 64,\n",
    "                \"adapter_id\": invoke_adapter_id,\n",
    "                \"adapter_source\": \"s3\"\n",
    "            }\n",
    "        }\n",
    "        start_time = time.time()\n",
    "        response_adapter = lorax_predictor.predict(payload_adapter)\n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    aggregate_latency.extend(latencies)\n",
    "\n",
    "def benchmark_scenario(single_adapter=False):\n",
    "    threads = []\n",
    "    all_latencies = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(num_threads):\n",
    "        thread_latencies = []\n",
    "        all_latencies.append(thread_latencies)\n",
    "        thread = threading.Thread(target=invoke_adapter, args=(thread_latencies, single_adapter))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    total_latency = sum([sum(latencies) for latencies in all_latencies])\n",
    "    total_requests_made = sum([len(latencies) for latencies in all_latencies])\n",
    "    average_latency = total_latency / total_requests_made\n",
    "    throughput = total_requests_made / (time.time() - start_time)\n",
    "\n",
    "    print(f\"Total Time: {time.time() - start_time}s\")\n",
    "    print(f\"Average Latency: {average_latency}s\")\n",
    "    print(f\"Throughput: {throughput} requests/s\")\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Benchmarking: Single Adapter Multiple Times\")\n",
    "benchmark_scenario(single_adapter=True)\n",
    "\n",
    "print(\"\\nBenchmarking: Multiple Adapters with Random Access\")\n",
    "benchmark_scenario()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9521-f5ed-487b-ba84-5a834add3d78",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "## Cleanup endpoint resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920da7-c0ce-4f3f-9eea-297abb49b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorax_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
